{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "기계학습(AAI107)_Team13_금융뉴스_감성분석_모델확인_Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MunJinSeo/MachineLearning/blob/main/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5(AAI107)_Team13_%EA%B8%88%EC%9C%B5%EB%89%B4%EC%8A%A4_%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D_%EB%AA%A8%EB%8D%B8%ED%99%95%EC%9D%B8_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 금융 뉴스 감정 분석 : 모델 동작 확인 Test\n",
        "- 사용 모델: snunlp/KR-FinBert-SC\n",
        "- https://huggingface.co/snunlp/KR-FinBert-SC\n",
        "- https://github.com/snunlp/KR-FinBert-SC"
      ],
      "metadata": {
        "id": "AWpNPgQNiYQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기본 환경 설치"
      ],
      "metadata": {
        "id": "aN5C-EPRwCzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yInx6XCD3xq",
        "outputId": "9cbaeff1-faab-49da-e1da-738cfa9f989e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification # pre-trained된 모델 로딩\n",
        "import torch\n",
        "\n",
        "import urllib.request # 데이터 파일 URL다운로드\n",
        "from tqdm import tqdm #모델 예측시 계산 진행상태를 보여줌\n",
        "\n",
        "#from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset # 데이터셋을 분할하여 사용\n",
        "\n",
        "import gc #GPU 캐쉬 초기화"
      ],
      "metadata": {
        "id": "z40lyiYDELqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available() ) # GPU가용성 확인\n",
        "print(torch.cuda.current_device() ) #현재 선택한 장치의 인덱스 \n",
        "print(torch.cuda.device_count() ) #사용할 수 있는 GPU 수\n",
        "print(torch.cuda.get_device_name(0) ) #장치 이름"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CElqgDgpr9Mj",
        "outputId": "894b926e-d75d-4a05-94bf-fd18fabd1420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "0\n",
            "1\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU or CPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "  print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print('No GPU available, using the CPU instead.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1h5G1qMuJQn",
        "outputId": "9b58101c-0ede-4373-e0d9-3f6528ed427e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 캐쉬 비워줌\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "G7IAhrdfwpjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 준비"
      ],
      "metadata": {
        "id": "HS5GzgQVwSlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_test(Dataset):\n",
        "  def __init__(self, file_name):\n",
        "    # --- 전처리 1 start ------------------------\n",
        "    # 자료 출처 : 빅카인즈 https://www.bigkinds.or.kr/\n",
        "    #  빅카인즈 뉴스 자료는 해당 사이트에서 수작업으로 추출했으며, 해당 파일을 나의 github에 올려 놓은 후 아래 감성분석에 사용함)\n",
        "    #urllib.request.urlretrieve(\"https://raw.githubusercontent.com/MunJinSeo/MachineLearning/main/samsung_news_20180101-20181231.xlsx\", filename=\"samsung_news_2018.xlsx\")\n",
        "    \n",
        "    #self.dataset = pd.read_csv(file_name, sep=',')\n",
        "    test_data = [\"현대바이오, '폴리탁셀' 코로나19 치료 가능성에 19% 급등\" #긍정\n",
        "                 ,\"이수화학, 3분기 영업익 176억…전년比 80%↑\" #긍정\n",
        "                 ,\"GKL, 7년 만에 두 자릿수 매출성장 예상\" #긍정\n",
        "                 ,\"위지윅스튜디오, 콘텐츠 활약에 사상 첫 매출 1000억원 돌파\" #긍정\n",
        "                 ,\"삼성전자, 2년 만에 인도 스마트폰 시장 점유율 1위 '왕좌 탈환'\" #긍정\n",
        "                 ,\"영화관株 '코로나 빙하기' 언제 끝나나…CJ CGV 올 4000억 손실 날수도\" #부정\n",
        "                 ,\"C쇼크에 멈춘 흑자비행…대한항공 1분기 영업적자 566억\" #부정\n",
        "                 ,\"'1000억대 횡령·배임' 최신원 회장 구속… SK네트웍스 경영 공백 방지 최선\" #부정\n",
        "                 ,\"부품 공급 차질에…기아차 광주공장 전면 가동 중단\" #부정\n",
        "                 ,\"현대제철, 지난해 영업익 3,313억원···전년比 67.7% 감소\" #부정\n",
        "                 ]\n",
        "    self.dataset = pd.DataFrame({\"제목\":test_data})\n",
        "    \n",
        "\n",
        "    # tokenizer\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert-SC\")\n",
        "    \n",
        "    # --- 전처리 1 end ------------------------\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "  \n",
        "  def clean_text(self, txt):\n",
        "    # --- 전처리 2 start ----------------------\n",
        "    cleaned = self.sub1.sub('', txt.strip())  # .strip()은 문장의 앞뒤 공백제거함\n",
        "    cleaned = self.sub2.sub(' ', cleaned)\n",
        "    cleaned = self.sub3.sub('.', cleaned)\n",
        "    #cleaned = emoticon_normalize(cleaned, num_repeats=3) # 감정 반복 단순화\n",
        "    #cleaned = repeat_normalize(cleaned, num_repeats=2) # 중복 글자 단순화\n",
        "    #cleaned = only_text(cleaned) # text만 추출\n",
        "    #cleaned = only_hangle(cleaned) # 한글만 추출\n",
        "    #cleaned = only_hangle_number(cleaned) # 한글/숫자만 추출\n",
        "\n",
        "    #if len(ssStr) > 1:\n",
        "    cleaned = \"[CLS] \" + cleaned + \" [SEP]\"\n",
        "    # --- 전처리 2 end ------------------------\n",
        "    return cleaned\n",
        "\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    text = self.dataset['제목'][idx] #제목, 본문 각각 해봐야할듯함\n",
        "\n",
        "    inputs = self.tokenizer(\n",
        "        text, \n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        pad_to_max_length=True, #`pad_to_max_length` 인수는 더 이상 사용되지 않으며 향후 버전에서 제거. 일괄 처리에서 가장 긴 시퀀스로 채우려면 `padding=True` 또는 `padding='longest'`를 사용하거나 `padding='max_length'를 사용하세요. ` 최대 길이로 채웁니다. 이 경우 `max_length`로 특정 길이를 지정하거나(예: `max_length=45`) max_length를 None으로 두어 모델의 최대 입력 크기(예: Bert의 경우 512)로 채울 수 있습니다.\n",
        "        #padding=True ,\n",
        "        add_special_tokens=True\n",
        "        )\n",
        "\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "    attention_mask = inputs['attention_mask'][0]\n",
        "\n",
        "    return input_ids, attention_mask"
      ],
      "metadata": {
        "id": "TSvkpBXVT6HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_dataset = Dataset_test(\"\")\n",
        "sample_dataset.dataset"
      ],
      "metadata": {
        "id": "gIqFy1aE7NhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 생성 : snunlp/KR-FinBert-SC"
      ],
      "metadata": {
        "id": "krPmK475w3QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성 : snunlp/KR-FinBert-SC\n",
        "# https://huggingface.co/snunlp/KR-FinBert-SC\n",
        "# https://github.com/snunlp/KR-FinBert-SC\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"snunlp/KR-FinBert-SC\",num_labels=3).to(device)\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(\"snunlp/KR-FinBert\",num_labels=3)\n",
        "#model.cuda()"
      ],
      "metadata": {
        "id": "XoLf3wpf7mJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 로딩 및 배치사이즈 정의"
      ],
      "metadata": {
        "id": "ps0MkY32xYIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로딩\n",
        "batchSize = 30 # GPU 메모리 부족 오류나면 수치를 줄일것\n",
        "\n",
        "sample_loader = DataLoader(sample_dataset, batch_size=batchSize, shuffle=False)\n",
        "\n",
        "sample_result = sample_dataset.dataset.copy(deep=True)\n",
        "print(sample_result)"
      ],
      "metadata": {
        "id": "W6QaiNJt74vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 실행 및 감성예측"
      ],
      "metadata": {
        "id": "oVn5QreTxeQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() #모델 평가모드로 변경\n",
        "\n",
        "# GPU 캐쉬 비워줌\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "idx_s = 0\n",
        "idx_e = 0\n",
        "\n",
        "#label_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "print(model.config.id2label) # 모델의 분류 종류 보기\n",
        "\n",
        "#예측 결과 초기화\n",
        "sample_result['pred_label'] = 0 # 예측 분류값\n",
        "sample_result['negative'] = 0 # 부정적 감성 확률\n",
        "sample_result['neutral'] = 0 # 보통 확률\n",
        "sample_result['positive'] = 0 # 긍정적 감성 확률\n",
        "\n",
        "# 배치 사이즈별 예측 실행\n",
        "# tqdm은 진행상황을 표시해줌\n",
        "#for input_ids_batch, attention_masks_batch,y_batch in tqdm(sample_loader):\n",
        "for input_ids_batch, attention_masks_batch in tqdm(sample_loader):\n",
        "  #y_batch = y_batch.to(device)\n",
        "  y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
        "  _, predicted = torch.max(y_pred, 1) #예측한 원소중 max값 1개만 반환\n",
        "  predictions = torch.nn.functional.softmax(y_pred, dim=-1) # 예측한 원소의 값이 0과 1사이의 값으로 변환 (원소 합은 1)\n",
        "  #print(predictions)\n",
        "  \n",
        "  rsList = list(map(int, predicted)) # 결과를 한번에 저장하기 위해 LIST로 변환 처리\n",
        "  rs_negative = predictions[:, 0].tolist() #부정\n",
        "  rs_neutral = predictions[:, 1].tolist() #보통\n",
        "  rs_positive = predictions[:, 2].tolist() #긍정\n",
        "\n",
        "  print(rsList)\n",
        "  idx_e += len(rsList) #해당 배치구간내에 index 끝값 계산\n",
        "  idx_s = idx_e - len(rsList)\n",
        "  print(\"index==\", idx_s, idx_e)\n",
        "\n",
        "  sample_result['pred_label'][idx_s : idx_e] = rsList #배치사이즈 만큼 예측된 값 일괄 저장\n",
        "  sample_result['negative'][idx_s : idx_e] = rs_negative\n",
        "  sample_result['neutral'][idx_s : idx_e] = rs_neutral\n",
        "  sample_result['positive'][idx_s : idx_e] = rs_positive\n"
      ],
      "metadata": {
        "id": "SFIQlQrN8FoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_result)"
      ],
      "metadata": {
        "id": "c5H5GAHm8mT5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}